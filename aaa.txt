Program 1


import pandas as pd
import numpy as np

def find_s(con, tar):
    # Initialize the specific hypothesis to the first positive example
    for i, val in enumerate(tar):
        if val == 'yes':
            specific_h = con[i].copy()
            break
            
    # Loop through the positive examples and update the specific hypothesis
    for i, val in enumerate(con):
        if tar[i] == 'yes':
            for j in range(len(specific_h)):
                if val[j] != specific_h[j]:
                    specific_h[j] = '?'
                else:
                    pass
    
    return specific_h

def list_then_eliminate(con, tar):
    # Initialize the most general hypothesis
    general_h = ['?' for i in range(len(con[0]))]
    
    # Loop through the positive examples and eliminate any inconsistent attribute
    for i, val in enumerate(tar):
        if val == 'yes':
            for j in range(len(con[i])):
                if general_h[j] == '?':
                    general_h[j] = con[i][j]
                elif general_h[j] != con[i][j]:
                    general_h[j] = '?'
    
    return general_h


# Load the dataset from a CSV file
data = pd.read_csv('ENJOYSPORT.csv')
print(data) 

# Split the dataset into attributes and target variable
print("-----------")
concepts=np.array(data)[:,:-1]
print(concepts)
print("-----------")
targets=np.array(data)[:,-1]
print(targets)
print("-----------")
# Apply the FIND-S algorithm to find the specific hypothesis
h1 = find_s(concepts,targets)
print("Most specific hypothesis by FIND-S algorithm is:", h1)

# Apply the LIST THEN ELIMINATE algorithm to find the most specific hypothesis
h2 = list_then_eliminate(concepts,targets)
print("Final hypothesis space by LIST THEN ELIMINATE algorithm is:", h2)

-----------------------------------------------------------------------------------------




Program 2


import numpy as np
import pandas as pd
data=pd.DataFrame(data=pd.read_csv('ENJOYSPORT.csv'))
print(data)
concepts=np.array(data.iloc[:,0:-1])
print(concepts)
target=np.array(data.iloc[:,-1])
print(target)
def learn(concepts, target):
    
    '''
    learn() function implements the learning method of the Candidate elimination algorithm.
    Arguments:
        concepts - a data frame with all the features
        target - a data frame with corresponding output values
    '''

    # Initialise S0 with the first instance from concepts
    # .copy() makes sure a new list is created instead of just pointing to the same memory location
    specific_h = concepts[0].copy()
    print("\nInitialization of specific_h and general_h")
    print(specific_h)
    #h=["#" for i in range(0,5)]
    #print(h)

    general_h = [["?" for i in range(len(specific_h))] for i in range(len(specific_h))]
    print(general_h)
    # The learning iterations
    for i, h in enumerate(concepts):

        # Checking if the hypothesis has a positive target
        if target[i] == "Yes":
            for x in range(len(specific_h)):

                # Change values in S & G only if values change
                if h[x] != specific_h[x]:
                    specific_h[x] = '?'
                    general_h[x][x] = '?'

        # Checking if the hypothesis has a positive target
        if target[i] == "No":
            for x in range(len(specific_h)):
                # For negative hyposthesis change values only  in G
                if h[x] != specific_h[x]:
                    general_h[x][x] = specific_h[x]
                else:
                    general_h[x][x] = '?'

        print("\nSteps of Candidate Elimination Algorithm",i+1)
        print(specific_h)
        print(general_h)
    
    # find indices where we have empty rows, meaning those that are unchanged
    indices = [i for i, val in enumerate(general_h) if val == ['?', '?', '?', '?', '?', '?']]
    for i in indices:
        # remove those rows from general_h
        general_h.remove(['?', '?', '?', '?', '?', '?'])
    # Return final values
    return specific_h, general_h

s_final, g_final = learn(concepts, target)
print("\nFinal Specific_h:", s_final, sep="\n")
print("\nFinal General_h:", g_final, sep="\n")



-------------------------------------------------------------------------------------------------

Program 3

import pandas as pd
data = pd.read_csv(r'C:\Users\91789\OneDrive\Desktop\DIP_Lab\MachineLearning\Housing1 (1).csv')
data

#Data pre processing

#data cleaning
# Check for missing values
print(data.isnull().sum())
# Remove rows with missing values
data = data.dropna()
data
#data integration
# Merge data from two datasets based on a common column
data = pd.read_csv(r'C:\Users\91789\OneDrive\Desktop\DIP_Lab\MachineLearning\Housing1 (1).csv')
data1 = pd.read_csv('Housing2.csv')
data_merged = pd.merge(data, data1, on='price')

# Concatenate two datasets vertically
data_concatenated = pd.concat([data, data1], axis=0)
data_concatenated

#Data transformation involves converting data into a suitable format or scale for analysis. Some common techniques include:
#Min-Max Scaling
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
data['offer'] = scaler.fit_transform(data['offer'].values.reshape(-1, 1))

# Standardization
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
data['offer'] = scaler.fit_transform(data['offer'].values.reshape(-1, 1))
data

#single column elimination
# Create DataFrame
df = pd.DataFrame(data)

# Print original dataset
print("Original dataset:")
print(df)
print()

# Delete columns with a single value
df = df.loc[:, df.nunique() > 1]

# Print preprocessed dataset
print("Preprocessed dataset:")
print(df)

#Eliminating duplicate rows in dataset
duplicate_rows = data[data.duplicated()]
duplicate_rows

data.drop_duplicates(inplace=True)
data.reset_index(drop=True, inplace=True)
data


------------------------------------------------------------------------------------------------


Program 4


import pandas as pd
import math

def base_entropy(dataset):
    p=0
    n=0
    target = dataset.iloc[:, -1]
    targets = list(set(target))
    for i in target:
        if i == targets[0]:
            p=p+1
        else:
            n = n + 1
    if p == 0 or n== 0:
        return 0
    elif p == n:
        return 1
    else:
        entropy=0-(((p/(p+n)) * (math.log2(p/ (p+n))) + (n/(p+ n)) * (math.log2(n/ (p + n)))))
    return entropy

#function to calculate the entropy of attributes
def entropy(dataset, feature, attribute):
    p = 0
    n = 0
    target = dataset.iloc[:, -1]
    targets = list(set(target))
    for i, j in zip(feature, target):
        if i == attribute and j == targets[0]:
            p = p + 1
        elif i == attribute and j == targets[1]:
            n = n + 1
    if p == 0 or n == 0:
        return 0
    elif p == n:
        return 1
    else:
        entropy = 0 - ((p/(p+n)) * (math.log2(p/ (p + n))) + (n/ (p+ n)) * (math.log2(n/ (p+ n))))
    return entropy


def counter(target, attribute, i):
    p = 0
    n = 0
    targets = list(set(target))
    for j, k in zip(target, attribute):
        if j== targets[0] and k == i:
            p = p + 1
        elif j== targets[1] and k == i:
            n = n + 1
    return p,n

# function that calculates the information gain

def Information_Gain(dataset, feature):
    Distinct = list(set(feature))
    Info_Gain=0
    for i in Distinct:
        Info_Gain= Info_Gain + feature.count(i) / len(feature)* entropy(dataset,feature ,i)
        Info_Gain= base_entropy(dataset) - Info_Gain
    return Info_Gain


#function that generates the childs of selected Attribute

def generate_childs(dataset, attribute_index):  
    distinct = list(dataset.iloc[:, attribute_index])
    childs = dict()
    for i in distinct:
        childs[i] = counter(dataset.iloc[:, -1], dataset.iloc[:, attribute_index], i)
    return childs

#function that modifies the dataset according to the impure childs

def modify_data_set(dataset,index, feature, impurity):
    size = len(dataset)
    subdata = dataset[dataset[feature] == impurity]
    del (subdata[subdata.columns[index]])
    return subdata

# function that return attribute with the greatest Information Gain
def greatest_information_gain(dataset):
    max=-1
    attribute_index = 0
    size = len(dataset.columns) - 1
    for i in range(0, size):
        feature= list(dataset.iloc[:, i])
        i_g= Information_Gain(dataset, feature)
        if max < i_g:
            max = i_g
            attribute_index = i
    return attribute_index

# function to construct the decision tree
def construct_tree(dataset, tree):
    target = dataset.iloc[:, -1]
    impure_childs = []
    attribute_index = greatest_information_gain(dataset)
    childs = generate_childs(dataset, attribute_index)
    tree[dataset.columns[attribute_index]] = childs
    targets = list(set(dataset.iloc[:, -1]))
    for k, v in childs.items():
        if v[0] == 0:
            tree[k] = targets[1]
        elif v[1] == 0:
            tree[k] = targets[0]
        elif v[0] != 0 or v[1] != 0:
            impure_childs.append(k)
    for i in impure_childs:
        sub = modify_data_set(dataset,attribute_index,dataset.columns[attribute_index], i)
        #if len(sub) > 0:
        tree = construct_tree(sub, tree)
    return tree

#main function
def main():
    df = pd.read_csv("ENJOYSPORT.csv")
    tree = dict()
    result = construct_tree(df, tree)
    for key, value in result.items(): 
        print(key, "=>", value)

if __name__ == "__main__":
    main() 




--------------------------------------------------------------------------------------------------

Program 5


import numpy as nm  
import matplotlib.pyplot as mtp  
import pandas as pd  
  
#importing datasets  
data_set= pd.read_csv('user_data.csv')  
  
#Extracting Independent and dependent Variable  
x= data_set.iloc[:, [2,3]].values  
y= data_set.iloc[:, 4].values  
  
# Splitting the dataset into training and test set.  
from sklearn.model_selection import train_test_split  
x_train, x_test, y_train, y_test= train_test_split(x, y, test_size= 0.25, random_state=0)  
  
#feature Scaling  
from sklearn.preprocessing import StandardScaler    
st_x= StandardScaler()    
x_train= st_x.fit_transform(x_train)    
x_test= st_x.transform(x_test)
#Fitting Decision Tree classifier to the training set  
from sklearn.ensemble import RandomForestClassifier  
classifier= RandomForestClassifier(n_estimators= 10, criterion="entropy")  
classifier.fit(x_train, y_train)
#Predicting the test set result  
y_pred= classifier.predict(x_test)
#Creating the Confusion matrix  
from sklearn.metrics import confusion_matrix  
cm= confusion_matrix(y_test, y_pred)
from matplotlib.colors import ListedColormap  
x_set, y_set = x_train, y_train  
x1, x2 = nm.meshgrid(nm.arange(start = x_set[:, 0].min() - 1, stop = x_set[:, 0].max() + 1, step  =0.01), nm.arange(start = x_set[:, 1].min() - 1, stop = x_set[:, 1].max() + 1, step = 0.01))  
mtp.contourf(x1, x2, classifier.predict(nm.array([x1.ravel(), x2.ravel()]).T).reshape(x1.shape),alpha = 0.75, cmap = ListedColormap(('purple','green' )))  
mtp.xlim(x1.min(), x1.max())  
mtp.ylim(x2.min(), x2.max())  
for i, j in enumerate(nm.unique(y_set)):  
    mtp.scatter(x_set[y_set == j, 0], x_set[y_set == j, 1],c = ListedColormap(('purple', 'green'))(i), label = j)  
mtp.title('Random Forest Algorithm (Training set)')  
mtp.xlabel('Age')  
mtp.ylabel('Estimated Salary')  
mtp.legend()  
mtp.show()

#Visulaizing the test set result  
from matplotlib.colors import ListedColormap  
x_set, y_set = x_test, y_test  
x1, x2 = nm.meshgrid(nm.arange(start = x_set[:, 0].min() - 1, stop = x_set[:, 0].max() + 1, step  =0.01),nm.arange(start = x_set[:, 1].min() - 1, stop = x_set[:, 1].max() + 1, step = 0.01))  
mtp.contourf(x1, x2, classifier.predict(nm.array([x1.ravel(), x2.ravel()]).T).reshape(x1.shape),alpha = 0.75, cmap = ListedColormap(('purple','green' )))  
mtp.xlim(x1.min(), x1.max())  
mtp.ylim(x2.min(), x2.max())  
for i, j in enumerate(nm.unique(y_set)):  
    mtp.scatter(x_set[y_set == j, 0], x_set[y_set == j, 1],c = ListedColormap(('purple', 'green'))(i), label = j)  
mtp.title('Random Forest Algorithm(Test set)')  
mtp.xlabel('Age')  
mtp.ylabel('Estimated Salary')  
mtp.legend()  
mtp.show()  



---------------------------------------------------------------------------------------------------


Program 6


import pandas as pd
from sklearn import tree
from sklearn.preprocessing import LabelEncoder
from sklearn.naive_bayes import GaussianNB
# Load Data from CSV
data = pd.read_csv('playtennis.csv')
print("The first 5 Values of data is :\n", data.head())
# obtain train data and train output
X = data.iloc[:, :-1]
print("\nThe First 5 values of the train data is\n", X.head())
y = data.iloc[:, -1]
print("\nThe First 5 values of train output is\n", y.head())
# convert them in numbers
le_outlook = LabelEncoder()
X.Outlook = le_outlook.fit_transform(X.Outlook)
le_Temperature = LabelEncoder()
X.Temperature = le_Temperature.fit_transform(X.Temperature)
le_Humidity = LabelEncoder()
X.Humidity = le_Humidity.fit_transform(X.Humidity)
le_Wind = LabelEncoder()
X.Wind = le_Wind.fit_transform(X.Wind)
print("\nNow the Train output is\n", X.head())
le_PlayTennis = LabelEncoder()
y = le_PlayTennis.fit_transform(y)
print("\nNow the Train output is\n",y)
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.20)
classifier = GaussianNB()
classifier.fit(X_train, y_train)
from sklearn.metrics import accuracy_score
print("Accuracy is:", accuracy_score(classifier.predict(X_test), y_test))




----------------------------------------------------------------------------------------------------

Program 7

import numpy as np
import csv
import pandas as pd
from pgmpy.models import BayesianNetwork
from pgmpy.estimators import MaximumLikelihoodEstimator
from pgmpy.inference import VariableElimination

#read Cleveland Heart Disease data
heartDisease = pd.read_csv('heart.csv')
heartDisease = heartDisease.replace('?',np.nan)

#display the data
print('Few examples from the dataset are given below')
print(heartDisease.head())

#Model Bayesian Network 
model=BayesianNetwork([('age','trestbps'),('age','fbs'),('sex','trestbps'),('exang','trestbps'),('trestbps','heartdisease'),('fbs','heartdisease'),('heartdisease','restecg'),('heartdisease','thalach'),('heartdisease','chol')])

#Learning CPDs using Maximum Likelihood Estimators
print('\n Learning CPD using Maximum likelihood estimators')
model.fit(heartDisease,estimator=MaximumLikelihoodEstimator)

# Inferencing with Bayesian Network
print('\n Inferencing with Bayesian Network:')
HeartDisease_infer = VariableElimination(model)

#computing the Probability of HeartDisease given Age
print('\n 1. Probability of HeartDisease given Age=30')
q=HeartDisease_infer.query(variables=['heartdisease'],evidence={'age': 53, 'sex' :1})
print(q)



----------------------------------------------------------------------------------------------------

Program 8

import numpy as np
import csv
import pandas as pd
from pgmpy.models import BayesianModel
from pgmpy.estimators import MaximumLikelihoodEstimator
from pgmpy.inference import VariableElimination
#read Cleveland Heart Disease data
heartDisease = pd.read_csv('heart.csv')
heartDisease = heartDisease.replace('?',np.nan)
#display the data
print('Few examples from the dataset are given below')
print(heartDisease.head())
#Model Bayesian Network
Model=BayesianModel([('age','trestbps'),('age','fbs'),('sex','trestbps'),('exang','trestbps'),('tres
tbps','heartdisease'),('fbs','heartdisease'),('heartdisease','restecg'),('heartdisease','thalach'),('h
eartdisease','chol')])
#Learning CPDs using Maximum Likelihood Estimators
print('\n Learning CPD using Maximum likelihood estimators')
model.fit(heartDisease,estimator=MaximumLikelihoodEstimator)
# Inferencing with Bayesian Network
print('\n Inferencing with Bayesian Network:')
HeartDisease_infer = VariableElimination(model)
#computing the Probability of HeartDisease given Age
print('\n 1. Probability of HeartDisease given Age=30')
q=HeartDisease_infer.query(variables=['heartdisease'],evidence={'age': 37, 'sex' :0})
print(q)



---------------------------------------------------------------------------------------------------

Program 9

from sklearn import datasets
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
import sklearn.metrics as metrics
import pandas as pd
import numpy as np
iris=datasets.load_iris()
x=pd.DataFrame(iris.data)
x.columns=['Sepal_length','Sepal_width','Petal_length','Petal_width']
y=pd.DataFrame(iris.target)
y.columns=['Targets']
model=KMeans(n_clusters=3)
model.fit(x)
plt.figure(figsize=(14,7))
colormap=np.array(['red','lime','black'])
plt.subplot(1,3,1)
plt.scatter(x.Petal_length,x.Petal_width,c=colormap[y.Targets],s=40)
plt.title('Real Cluster')
plt.xlabel('Petal length')
plt.subplot(1,3,2)
plt.scatter(x.Petal_length,x.Petal_width,c=colormap[model.labels_],s=40)
plt.title('KMeans Clustering')
plt.xlabel('Petal length')
plt.ylabel('Petal Width')
from sklearn import preprocessing
from sklearn.mixture import GaussianMixture
gmm=GaussianMixture(n_components=3, random_state=0).fit(x)
y_cluster_gmm=gmm.predict(x)
plt.subplot(1,3,3)
plt.title('GMM Clustering')
plt.xlabel('Petal length')
plt.ylabel('Petal Width')
plt.scatter(x.Petal_length,x.Petal_width,c=colormap[y_cluster_gmm])
print('Observation:The GMM using EM Algorithm based clustering matched the true labels more closely than kmeans.')




-----------------------------------------------------------------------------------------------------


Program 10



import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.svm import SVC

def plot_svm_boundary_with_title(model, X, y, title):
    # Function to plot SVM decision boundary and data points with a title
    # Custom plot function to allow setting the title separately

    # Extract support vectors
    sv = model.support_vectors_

    # Plot data points and decision boundary
    sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y, palette='seismic')
    plt.scatter(sv[:, 0], sv[:, 1], marker='o', color='k', s=100, label='Support Vectors')
    xlim = plt.gca().get_xlim()
    ylim = plt.gca().get_ylim()

    # Create grid to evaluate model
    xx = np.linspace(xlim[0], xlim[1], 30)
    yy = np.linspace(ylim[0], ylim[1], 30)
    YY, XX = np.meshgrid(yy, xx)
    xy = np.vstack([XX.ravel(), YY.ravel()]).T
    Z = model.decision_function(xy).reshape(XX.shape)

    # Plot decision boundary and margins
    plt.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], alpha=0.5, linestyles=['--', '-', '--'])

    # Set plot title
    plt.title(title)
    plt.legend()
    plt.show()

def visualize_svm(kernel='linear', C=1.0, gamma='auto', degree=3):
    df = pd.read_csv("mouse_viral_study - mouse_viral_study-Copy1.csv")

    y = df['Virus Present']
    X = df.drop('Virus Present', axis=1)

    model = SVC(kernel=kernel, C=C, gamma=gamma, degree=degree)
    model.fit(X, y)

    title = f"SVM with Kernel: {kernel}, C: {C}, gamma: {gamma}, degree: {degree}"
    plot_svm_boundary_with_title(model, X.values, y.values, title)

# Examples of using the function with different parameters
visualize_svm(kernel='linear', C=0.005)
visualize_svm(kernel='linear', C=1)
visualize_svm(kernel='rbf', C=1)
visualize_svm(kernel='sigmoid')
visualize_svm(kernel='poly', C=1, degree=2)
visualize_svm(kernel='poly', C=1, degree=3)
visualize_svm(kernel='rbf', C=1, gamma='auto')




-----------------------------------------------------------------------------------------------------
